import os
import re
import json
import markdown
import shutil
from github import Github
from datetime import datetime, timedelta
from jinja2 import Environment, FileSystemLoader

# --- é…ç½®å€ ---
GITHUB_TOKEN = os.getenv("G_TT")
REPO_NAME = "myogg/Gitblog"
MAX_PER_CATEGORY = 5
CACHE_FILE = "github_cache.json"
CACHE_DURATION = 86400  # 6å°æ—¶
ARTICLES_DIR = "articles"
STATE_FILE = "generation_state.json"  # è®°å½•ç”ŸæˆçŠ¶æ€

# åˆå§‹åŒ– Jinja2 æ¨¡æ¿å¼•æ“
env = Environment(loader=FileSystemLoader('templates'))

def load_generation_state():
    """åŠ è½½ç”ŸæˆçŠ¶æ€"""
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    return {}

def save_generation_state(state):
    """ä¿å­˜ç”ŸæˆçŠ¶æ€"""
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

def needs_regeneration(issue, state):
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ"""
    article_file = os.path.join(ARTICLES_DIR, f"article-{issue.number}.html")

    # 1. æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ç”Ÿæˆ
    if not os.path.exists(article_file):
        return True

    # 2. æ£€æŸ¥çŠ¶æ€è®°å½•
    issue_key = str(issue.number)
    if issue_key not in state:
        return True

    # 3. æ¯”è¾ƒæ›´æ–°æ—¶é—´ï¼ˆå¦‚æœæœ‰ updated_at å±æ€§ï¼‰
    if hasattr(issue, 'updated_at'):
        last_generated = state[issue_key].get('generated_at')
        if last_generated:
            try:
                last_gen_time = datetime.fromisoformat(last_generated)
                if issue.updated_at > last_gen_time:
                    return True
            except Exception:
                return True

    return False

def get_text_color(hex_color):
    """æ ¹æ“šèƒŒæ™¯è‰²äº®åº¦æ±ºå®šæ–‡å­—é¡è‰²"""
    try:
        # ç¡®ä¿hex_coloræ˜¯6ä½æ ¼å¼
        hex_color = hex_color.lstrip('#')
        if len(hex_color) == 3:
            hex_color = ''.join([c*2 for c in hex_color])
        r, g, b = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)
        brightness = (r * 299 + g * 587 + b * 114) / 1000
        return "#ffffff" if brightness < 128 else "#000000"
    except Exception:
        return "#000000"

def generate_safe_name(label_name, existing_names=None):
    """ç”Ÿæˆå”¯ä¸€çš„ safe_name"""
    if existing_names is None:
        existing_names = set()

    # 1. å°†éå­—æ¯æ•°å­—å­—ç¬¦æ›¿æ¢ä¸ºçŸ­æ¨ªçº¿
    safe = re.sub(r'[^a-zA-Z0-9]', '-', label_name).lower()
    # 2. åˆå¹¶è¿ç»­çš„çŸ­æ¨ªçº¿ä¸ºä¸€ä¸ª
    safe = re.sub(r'-+', '-', safe)
    # 3. å»é™¤é¦–å°¾çš„çŸ­æ¨ªçº¿
    safe = safe.strip('-')

    # 4. å¦‚æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not safe:
        safe = 'label'

    # 5. ç¡®ä¿å”¯ä¸€æ€§ï¼šå¦‚æœå·²å­˜åœ¨ï¼Œæ·»åŠ æ•°å­—åç¼€
    original_safe = safe
    counter = 1
    while safe in existing_names:
        safe = f"{original_safe}-{counter}"
        counter += 1

    return safe

def extract_content_tags(body):
    """ä»æ–‡ç« å†…å®¹ä¸­æå–æ ‡ç­¾ï¼ˆä¸åˆ›å»ºGitHub Labelï¼‰

    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. ç®€å•æ ¼å¼: tags: Python, AI, æœºå™¨å­¦ä¹ 
    2. YAML frontmatter:
       ---
       tags: Python, AI, æœºå™¨å­¦ä¹ 
       ---

    è¿”å›: (æ ‡ç­¾åˆ—è¡¨, æ¸…ç†åçš„å†…å®¹)
    """
    if not body:
        return [], body

    tags = []
    cleaned_body = body

    # æ–¹å¼1: æ£€æŸ¥YAML frontmatteræ ¼å¼
    yaml_pattern = r'^---\s*\n(.*?)\n---\s*\n'
    yaml_match = re.match(yaml_pattern, body, re.DOTALL)

    if yaml_match:
        frontmatter = yaml_match.group(1)
        # æå–tagsè¡Œ
        tags_match = re.search(r'tags:\s*(.+)', frontmatter, re.IGNORECASE)
        if tags_match:
            tags_str = tags_match.group(1).strip()
            # åˆ†å‰²æ ‡ç­¾ï¼ˆæ”¯æŒé€—å·æˆ–ä¸­æ–‡é€—å·åˆ†éš”ï¼‰
            tags = [t.strip() for t in re.split(r'[,ï¼Œ]\s*', tags_str) if t.strip()]
        # ç§»é™¤frontmatter
        cleaned_body = re.sub(yaml_pattern, '', body, count=1)

    # æ–¹å¼2: æ£€æŸ¥ç®€å•æ ¼å¼ï¼ˆæ–‡ç« å¼€å¤´çš„tags: ...ï¼‰
    else:
        simple_pattern = r'^tags:\s*(.+?)(?:\n|$)'
        simple_match = re.match(simple_pattern, body, re.IGNORECASE | re.MULTILINE)

        if simple_match:
            tags_str = simple_match.group(1).strip()
            # åˆ†å‰²æ ‡ç­¾
            tags = [t.strip() for t in re.split(r'[,ï¼Œ]\s*', tags_str) if t.strip()]
            # ç§»é™¤tagsè¡Œ
            cleaned_body = re.sub(simple_pattern, '', body, count=1).lstrip()

    return tags, cleaned_body

def extract_summary(body):
    """æå–æ‰‹åŠ¨è®¾ç½®çš„æ–‡ç« æ‘˜è¦ï¼ˆä»…æ”¯æŒ <!-- more --> åˆ†éš”ç¬¦ï¼‰"""
    if not body:
        return None

    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨æ‘˜è¦åˆ†éš”ç¬¦
    if "<!-- more -->" in body:
        summary = body.split("<!-- more -->")[0].strip()
        return summary if summary else None

    # æ²¡æœ‰åˆ†éš”ç¬¦åˆ™ä¸æ˜¾ç¤ºæ‘˜è¦
    return None

def add_lazy_loading_to_images(html_content):
    """ä¸ºå¤–éƒ¨å›¾åºŠå›¾ç‰‡æ·»åŠ æ‡’åŠ è½½å’Œä¼˜åŒ–åŠŸèƒ½"""
    if not html_content:
        return html_content

    # åŒ¹é…æ‰€æœ‰imgæ ‡ç­¾
    img_pattern = r'<img\s+([^>]*?)src="([^"]+)"([^>]*?)>'

    def replace_img(match):
        before_src = match.group(1)
        src_url = match.group(2)
        after_src = match.group(3)

        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰loadingå±æ€§
        if 'loading=' in before_src or 'loading=' in after_src:
            return match.group(0)  # å·²æœ‰loadingå±æ€§ï¼Œä¸ä¿®æ”¹

        # æ·»åŠ æ‡’åŠ è½½ã€å ä½ç¬¦å’Œé”™è¯¯å¤„ç†
        # ä½¿ç”¨data-srcå­˜å‚¨çœŸå®URLï¼Œå…ˆæ˜¾ç¤ºå ä½ç¬¦
        new_img = (
            f'<img {before_src}'
            f'data-src="{src_url}" '
            f'src="data:image/svg+xml,%3Csvg xmlns=\'http://www.w3.org/2000/svg\' viewBox=\'0 0 1 1\'%3E%3C/svg%3E" '
            f'loading="lazy" '
            f'class="lazyload img-loading" '
            f'alt="" '
            f'{after_src}>'
        )
        return new_img

    # æ›¿æ¢æ‰€æœ‰imgæ ‡ç­¾
    result = re.sub(img_pattern, replace_img, html_content)
    return result

def find_prev_next_articles(current_issue, all_issues):
    """æŸ¥æ‰¾ä¸Šä¸€ç¯‡å’Œä¸‹ä¸€ç¯‡æ–‡ç« ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰"""
    # è¿‡æ»¤æ‰pinnedæ–‡ç« ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    sorted_issues = sorted(
        [i for i in all_issues if not any(l.name.lower() == "pinned" for l in i.labels)],
        key=lambda x: x.created_at,
        reverse=True
    )

    try:
        current_index = next(i for i, issue in enumerate(sorted_issues) if issue.number == current_issue.number)
    except StopIteration:
        return None, None

    # ä¸Šä¸€ç¯‡ = æ—¶é—´æ›´æ—©çš„ (ç´¢å¼•+1)
    prev_article = sorted_issues[current_index + 1] if current_index + 1 < len(sorted_issues) else None
    # ä¸‹ä¸€ç¯‡ = æ—¶é—´æ›´æ™šçš„ (ç´¢å¼•-1)
    next_article = sorted_issues[current_index - 1] if current_index - 1 >= 0 else None

    return prev_article, next_article

def find_related_articles(current_issue, all_issues, max_count=2):
    """åŸºäºæ ‡ç­¾ç›¸ä¼¼åº¦æ¨èç›¸å…³æ–‡ç« """
    current_labels = set(l.name for l in current_issue.labels if l.name.lower() != "pinned")

    if not current_labels:
        return []

    # è®¡ç®—æ¯ç¯‡æ–‡ç« çš„ç›¸ä¼¼åº¦åˆ†æ•°
    related = []
    for issue in all_issues:
        if issue.number == current_issue.number:
            continue

        issue_labels = set(l.name for l in issue.labels if l.name.lower() != "pinned")

        # è®¡ç®—æ ‡ç­¾äº¤é›†æ•°é‡ä½œä¸ºç›¸ä¼¼åº¦
        common_labels = current_labels & issue_labels
        if common_labels:
            score = len(common_labels)
            related.append((issue, score))

    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆç›¸åŒåˆ†æ•°æŒ‰æ—¶é—´å€’åºï¼‰ï¼Œå–å‰max_countç¯‡
    related.sort(key=lambda x: (x[1], x[0].created_at), reverse=True)
    return [issue for issue, score in related[:max_count]]

def login():
    if not GITHUB_TOKEN:
        print("Error: G_TT token not found.")
        exit(1)
    return Github(GITHUB_TOKEN)

def fetch_issues(repo):
    """ç²å–ä¸¦ç·©å­˜ Issues"""
    # æª¢æŸ¥ç·©å­˜
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                cache_time = datetime.fromisoformat(cache_data['timestamp'])
                if datetime.now() - cache_time < timedelta(seconds=CACHE_DURATION):
                    print("ä½¿ç”¨ç·©å­˜æ•¸æ“š...")
                    # å‰µå»ºæ¨¡æ“¬Issueå°è±¡
                    issues = []
                    for item in cache_data['issues']:
                        # å‰µå»ºå‹•æ…‹å°è±¡ï¼ˆæ·»åŠ  updated_at å±æ€§ï¼‰
                        issue = type('Issue', (), {
                            'number': item['number'],
                            'title': item['title'],
                            'body': item['body'],
                            'created_at': datetime.fromisoformat(item['created_at']),
                            'updated_at': datetime.fromisoformat(item.get('updated_at', item['created_at'])),
                            'labels': [type('Label', (), {
                                'name': l['name'],
                                'color': l['color']
                            })() for l in item['labels']]
                        })()
                        issues.append(issue)
                    return issues
        except Exception as e:
            print(f"ç·©å­˜è®€å–å¤±æ•—: {e}")

    print("å¾GitHubç²å–æœ€æ–°æ•¸æ“š...")
    all_issues = [i for i in repo.get_issues(state="open") if not i.pull_request]

    # ä¿å­˜ç·©å­˜ï¼ˆæ·»åŠ  updated_atï¼‰
    cache_data = {
        'timestamp': datetime.now().isoformat(),
        'issues': [{
            'number': i.number,
            'title': i.title,
            'body': i.body,
            'created_at': i.created_at.isoformat(),
            'updated_at': i.updated_at.isoformat() if hasattr(i, 'updated_at') else i.created_at.isoformat(),
            'labels': [{'name': l.name, 'color': l.color} for l in i.labels]
        } for i in all_issues]
    }

    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ç·©å­˜å·²ä¿å­˜: {CACHE_FILE}")
    except Exception as e:
        print(f"ç·©å­˜ä¿å­˜å¤±æ•—: {e}")

    return all_issues

def sort_issues(issue_list):
    """æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰"""
    return sorted(issue_list, key=lambda x: x.created_at, reverse=True)

def generate_search_index(issues):
    """ç”Ÿæˆæœç´¢ç´¢å¼•"""
    search_data = []
    for issue in issues:
        # æå–å†…å®¹æ ‡ç­¾
        content_tags, cleaned_body = extract_content_tags(issue.body)

        # æ¸…ç†HTMLæ ‡ç­¾ï¼Œè·å–çº¯æ–‡æœ¬å†…å®¹
        clean_content = re.sub(r'<[^>]+>', '', cleaned_body or "")
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        clean_content = re.sub(r'\s+', ' ', clean_content).strip()

        # åˆå¹¶GitHubæ ‡ç­¾å’Œå†…å®¹æ ‡ç­¾
        all_tags = [label.name for label in issue.labels if label.name.lower() != "pinned"] + content_tags

        search_data.append({
            'id': issue.number,
            'title': issue.title,
            'content': clean_content[:500],  # æˆªå–å‰500å­—ç¬¦
            'date': issue.created_at.strftime('%Y-%m-%d'),
            'url': f'articles/article-{issue.number}.html',
            'tags': all_tags
        })

    # ä¿å­˜åˆ°staticç›®å½•
    os.makedirs("static", exist_ok=True)
    output_path = os.path.join("static", "search-index.json")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(search_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ æœç´¢ç´¢å¼•å·²ç”Ÿæˆ: {output_path}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæœç´¢ç´¢å¼•å¤±æ•—: {e}")

    return search_data

def generate_sitemap(issues):
    """ç”Ÿæˆ sitemap.xml"""
    try:
        base_url = "https://134688.xyz"

        # XML å¤´éƒ¨
        xml_content = ['<?xml version="1.0" encoding="UTF-8"?>']
        xml_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

        # ä¸»é¡µ
        xml_content.append('  <url>')
        xml_content.append(f'    <loc>{base_url}/</loc>')
        xml_content.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        xml_content.append('    <changefreq>daily</changefreq>')
        xml_content.append('    <priority>1.0</priority>')
        xml_content.append('  </url>')

        # æœç´¢é¡µ
        xml_content.append('  <url>')
        xml_content.append(f'    <loc>{base_url}/search.html</loc>')
        xml_content.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        xml_content.append('    <changefreq>weekly</changefreq>')
        xml_content.append('    <priority>0.8</priority>')
        xml_content.append('  </url>')

        # å…³äºé¡µ
        xml_content.append('  <url>')
        xml_content.append(f'    <loc>{base_url}/about.html</loc>')
        xml_content.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        xml_content.append('    <changefreq>monthly</changefreq>')
        xml_content.append('    <priority>0.7</priority>')
        xml_content.append('  </url>')

        # æ–‡ç« é¡µé¢
        for issue in issues:
            xml_content.append('  <url>')
            xml_content.append(f'    <loc>{base_url}/articles/article-{issue.number}.html</loc>')
            xml_content.append(f'    <lastmod>{issue.created_at.strftime("%Y-%m-%d")}</lastmod>')
            xml_content.append('    <changefreq>monthly</changefreq>')
            xml_content.append('    <priority>0.9</priority>')
            xml_content.append('  </url>')

        xml_content.append('</urlset>')

        # å†™å…¥æ–‡ä»¶
        with open("sitemap.xml", "w", encoding="utf-8") as f:
            f.write('\n'.join(xml_content))

        print(f"âœ“ Sitemap å·²ç”Ÿæˆ: sitemap.xml ({len(issues)} ç¯‡æ–‡ç« )")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆ Sitemap å¤±æ•—: {e}")

def generate_robots_txt():
    """ç”Ÿæˆ robots.txt"""
    try:
        robots_content = [
            "# robots.txt for 134688.xyz",
            "User-agent: *",
            "Allow: /",
            "",
            "# Sitemap location",
            "Sitemap: https://134688.xyz/sitemap.xml",
            "",
            "# Crawl-delay for polite crawling",
            "Crawl-delay: 1"
        ]

        with open("robots.txt", "w", encoding="utf-8") as f:
            f.write('\n'.join(robots_content))

        print("âœ“ robots.txt å·²ç”Ÿæˆ")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆ robots.txt å¤±æ•—: {e}")

def generate_article_page(issue, all_issues, giscus_config=None, label_info=None):
    """ç”Ÿæˆæ–‡ç« é¡µé¢ï¼Œè¿”å›ç”ŸæˆçŠ¶æ€ä¿¡æ¯"""
    try:
        os.makedirs(ARTICLES_DIR, exist_ok=True)
        template = env.get_template('article.html')

        # å…ˆæå–å†…å®¹æ ‡ç­¾
        content_tags, cleaned_body = extract_content_tags(issue.body)

        # è½¬æ¢Markdownä¸ºHTMLï¼ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼‰
        html_content = markdown.markdown(
            cleaned_body or "æš«ç„¡å…§å®¹",
            extensions=['extra', 'codehilite', 'tables', 'fenced_code']
        )

        # æ·»åŠ å›¾ç‰‡æ‡’åŠ è½½
        html_content = add_lazy_loading_to_images(html_content)

        # å¤„ç†GitHubæ ‡ç­¾
        existing_safe_names = set()
        labels_data = []
        for l in issue.labels:
            if l.name.lower() != "pinned":
                safe_name = generate_safe_name(l.name, existing_safe_names)
                existing_safe_names.add(safe_name)
                # å°è¯•ä» label_info è·å– safe_name
                if label_info and l.name in label_info:
                    safe_name = label_info[l.name].get('safe_name', safe_name)
                labels_data.append({
                    "name": l.name,
                    "color": l.color,
                    "text_color": get_text_color(l.color),
                    "safe_name": safe_name
                })

        # å¤„ç†å†…å®¹æ ‡ç­¾ï¼ˆä» label_info è·å– safe_nameï¼‰
        content_tags_data = []
        for tag in content_tags:
            tag_data = {"name": tag}
            if label_info and tag in label_info:
                tag_data["safe_name"] = label_info[tag].get('safe_name')
            content_tags_data.append(tag_data)

        # é»˜è®¤çš„Giscusé…ç½®
        default_giscus_config = {
            'repo': 'myogg/Gitblog',
            'repo_id': os.getenv('GISCUS_REPO_ID', ''),  # ä»ç¯å¢ƒå˜é‡è·å–
            'category': 'General',
            'category_id': os.getenv('GISCUS_CATEGORY_ID', ''),
            'theme': 'light',
            'lang': 'zh-CN'
        }

        # ä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–é»˜è®¤é…ç½®
        giscus_config = giscus_config or default_giscus_config

        # æ£€æŸ¥Giscusé…ç½®
        if not giscus_config.get('repo_id'):
            print(f"âš ï¸ æ–‡ç«  #{issue.number}: Giscus repo_id æœªé…ç½®ï¼Œè¯„è®ºç³»ç»Ÿå°†ä¸å¯ç”¨")

        # æŸ¥æ‰¾ä¸Šä¸‹æ–‡å’Œç›¸å…³æ–‡ç« 
        prev_article, next_article = find_prev_next_articles(issue, all_issues)
        related_articles = find_related_articles(issue, all_issues)

        # æ¸²æŸ“æ¨¡æ¿
        output = template.render(
            issue=issue,
            content=html_content,
            labels_data=labels_data,
            content_tags=content_tags_data,
            prev_article=prev_article,
            next_article=next_article,
            related_articles=related_articles,
            YEAR=datetime.now().year,
            giscus_config=giscus_config,
            base_path="../"
        )

        # å†™å…¥æ–‡ä»¶
        output_file = os.path.join(ARTICLES_DIR, f"article-{issue.number}.html")
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(output)

        print(f"âœ“ ç”Ÿæˆæ–‡ç« : {issue.title} (#{issue.number})")

        # è¿”å›çŠ¶æ€ä¿¡æ¯
        return {
            'generated_at': datetime.now().isoformat(),
            'updated_at': issue.updated_at.isoformat() if hasattr(issue, 'updated_at') else None,
            'title': issue.title
        }

    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡ç«  #{issue.number} å¤±è´¥: {e}")
        return None

def main():
    print("é–‹å§‹ç”ŸæˆGitBlogé é¢...")
    print(f"å€‰åº«: {REPO_NAME}")

    # æª¢æŸ¥æ¨¡æ¿ç›®éŒ„
    if not os.path.exists('templates'):
        print("âŒ éŒ¯èª¤: templates ç›®éŒ„ä¸å­˜åœ¨")
        exit(1)

    required_templates = ['base.html', 'article.html']
    for template in required_templates:
        if not os.path.exists(f'templates/{template}'):
            print(f"âŒ éŒ¯èª¤: ç¼ºå°‘å¿…è¦æ¨¡æ¿æ–‡ä»¶ {template}")
            exit(1)

    g = login()
    repo = g.get_repo(REPO_NAME)
    issues = fetch_issues(repo)

    if not issues:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•issues")
        exit(1)

    print(f"æ‰¾åˆ° {len(issues)} å€‹issues")

    # ä¸ºæ‰€æœ‰æ–‡ç« æ·»åŠ æ‘˜è¦å­—æ®µï¼ˆå¿…é¡»åœ¨ä½¿ç”¨å‰æ·»åŠ ï¼‰
    # å…ˆæå– tags å¹¶æ¸…ç†ï¼Œå†ä»æ¸…ç†åçš„ body æå–æ‘˜è¦
    for issue in issues:
        tags, cleaned_body = extract_content_tags(issue.body)
        issue.summary = extract_summary(cleaned_body)

    # --- æ•¸æ“šæ•´ç† ---
    label_dict = {}
    label_info = {}
    articles_by_year = {}

    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ ‡ç­¾å¹¶ç”Ÿæˆå”¯ä¸€çš„ safe_nameï¼ŒåŒæ—¶æŒ‰å¹´ä»½åˆ†ç»„
    existing_safe_names = set()
    for issue in issues:
        year = issue.created_at.strftime('%Y')
        articles_by_year.setdefault(year, []).append(issue)
        for label in issue.labels:
            if label.name.lower() == "pinned":
                continue
            label_dict.setdefault(label.name, []).append(issue)
            if label.name not in label_info:
                safe_name = generate_safe_name(label.name, existing_safe_names)
                existing_safe_names.add(safe_name)
                label_info[label.name] = {
                    "color": label.color,
                    "text_color": get_text_color(label.color),
                    "safe_name": safe_name
                }

    # æ’åºæ¯ä¸ªå¹´ä»½ä¸‹çš„æ–‡ç« ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
    for year in articles_by_year:
        articles_by_year[year] = sort_issues(articles_by_year[year])

    # æ’åºæ¯å€‹æ¨™ç±¤ä¸‹çš„æ–‡ç« 
    for label in label_dict:
        label_dict[label] = sort_issues(label_dict[label])

    sorted_years = sorted(articles_by_year.keys(), reverse=True)
    print(f"æ–‡ç« å¹´ä»½åˆ†ä½ˆ: {', '.join(sorted_years)}")
    print(f"æ‰¾åˆ° {len(label_dict)} å€‹æ¨™ç±¤")

    # --- ç”Ÿæˆæœç´¢ç´¢å¼• ---
    generate_search_index(issues)

    # --- é…ç½®Giscusè¯„è®ºç³»ç»Ÿ ---
    giscus_config = {
        'repo': 'myogg/Gitblog',
        'repo_id': os.getenv('GISCUS_REPO_ID', ''),
        'category': 'Announcements',
        'category_id': os.getenv('GISCUS_CATEGORY_ID', ''),
        'theme': 'light',
        'lang': 'zh-CN'
    }

    # æª¢æŸ¥Giscusé…ç½®
    if not giscus_config['repo_id']:
        print("âš ï¸ æ³¨æ„: Giscus repo_id æœªé…ç½®ï¼Œè¯„è®ºç³»ç»Ÿå°†ä¸å¯ç”¨")
        print("  è¯·è®¿é—® https://giscus.app è·å–é…ç½®ï¼Œå¹¶è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  GISCUS_REPO_ID=ä½ çš„repo_id")
        print("  GISCUS_CATEGORY_ID=ä½ çš„category_id")

    # --- æ”¶é›†å†…å®¹æ ‡ç­¾ï¼ˆåœ¨ç”Ÿæˆæ–‡ç« å‰ï¼‰ ---
    print("æ”¶é›†å…§å®¹æ¨™ç±¤...")
    content_tags_dict = {}  # {tag_name: [issue1, issue2, ...]}
    for issue in issues:
        content_tags, _ = extract_content_tags(issue.body)
        for tag in content_tags:
            if tag not in content_tags_dict:
                content_tags_dict[tag] = []
            content_tags_dict[tag].append(issue)

    # ä¸ºå†…å®¹æ ‡ç­¾ç”Ÿæˆ safe_nameï¼ˆæ·»åŠ åˆ° label_info ä¸­ï¼‰
    for tag in content_tags_dict:
        if tag not in label_info:  # é¿å…ä¸GitHubæ ‡ç­¾é‡å
            safe_name = generate_safe_name(tag, existing_safe_names)
            existing_safe_names.add(safe_name)
            label_info[tag] = {
                "color": None,  # å†…å®¹æ ‡ç­¾æ²¡æœ‰é¢œè‰²
                "text_color": None,
                "safe_name": safe_name
            }
        # æ’åºè¯¥æ ‡ç­¾ä¸‹çš„æ–‡ç« 
        content_tags_dict[tag] = sort_issues(content_tags_dict[tag])

    print(f"æ‰¾åˆ° {len(content_tags_dict)} å€‹å…§å®¹æ¨™ç±¤")

    # --- ç”Ÿæˆæ–‡ç« é é¢ï¼ˆå¢é‡ç”Ÿæˆï¼‰ ---
    print("é–‹å§‹ç”Ÿæˆæ–‡ç« é é¢...")
    state = load_generation_state()
    generated_count = 0
    skipped_count = 0
    updated_state = {}

    for issue in issues:
        issue_key = str(issue.number)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
        if needs_regeneration(issue, state):
            result = generate_article_page(issue, issues, giscus_config, label_info)
            if result:
                updated_state[issue_key] = result
                generated_count += 1
        else:
            # è·³è¿‡ï¼Œä¿ç•™æ—§çŠ¶æ€
            updated_state[issue_key] = state[issue_key]
            skipped_count += 1
            print(f"â­ï¸ è·³è¿‡æ–‡ç« : {issue.title} (#{issue.number}) - æ— æ›´æ–°")

    # ä¿å­˜çŠ¶æ€
    save_generation_state(updated_state)
    print(f"\nğŸ“Š æ–‡ç« ç”Ÿæˆç»Ÿè®¡: ç”Ÿæˆ {generated_count} ç¯‡, è·³è¿‡ {skipped_count} ç¯‡\n")

    # --- ç”Ÿæˆæ ‡ç­¾é¡µé¢ ---
    print("é–‹å§‹ç”Ÿæˆæ¨™ç±¤é é¢...")
    os.makedirs("tags", exist_ok=True)

    # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
    all_tags_dict = {**label_dict, **content_tags_dict}

    for tag_name, tag_articles in all_tags_dict.items():
        try:
            tag_template = env.get_template('tag.html')
            tag_info = label_info.get(tag_name, {})
            tag_html = tag_template.render(
                tag_name=tag_name,
                tag_color=tag_info.get('color'),
                articles=tag_articles,
                YEAR=datetime.now().year,
                base_path="../"
            )

            safe_name = tag_info.get('safe_name', generate_safe_name(tag_name, set()))
            output_file = os.path.join("tags", f"{safe_name}.html")
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(tag_html)
            print(f"âœ“ ç”Ÿæˆæ¨™ç±¤é : {tag_name} ({len(tag_articles)} ç¯‡æ–‡ç« )")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¨™ç±¤é  '{tag_name}' å¤±æ•—: {e}")

    # --- ç”Ÿæˆæ ‡ç­¾æ€»è§ˆé¡µé¢ --- (å·²ç¦ç”¨ï¼Œç”¨æˆ·ä¸éœ€è¦å¯¼èˆªæ æ ‡ç­¾å…¥å£)
    # print("ç”Ÿæˆæ¨™ç±¤ç¸½è¦½é ...")
    # try:
    #     tags_template = env.get_template('tags.html')
    #     ...
    # except Exception as e:
    #     print(f"âŒ ç”Ÿæˆæ¨™ç±¤ç¸½è¦½é å¤±æ•—: {e}")

    # 1. ç”Ÿæˆä¸»é ï¼ˆå¸¦åˆ†é¡µï¼‰
    print("ç”Ÿæˆä¸»é ...")
    try:
        # è·å–æ‰€æœ‰éç½®é¡¶æ–‡ç« ï¼ŒæŒ‰æ—¶é—´å€’åº
        all_articles = sorted(
            [i for i in issues if not any(l.name.lower() == "pinned" for l in i.labels)],
            key=lambda x: x.created_at,
            reverse=True
        )

        # åˆ†é¡µè®¾ç½®
        articles_per_page = 20
        total_articles = len(all_articles)
        total_pages = (total_articles + articles_per_page - 1) // articles_per_page

        index_template = env.get_template('base.html')

        # ç”Ÿæˆæ‰€æœ‰åˆ†é¡µ
        for page_num in range(1, total_pages + 1):
            start_idx = (page_num - 1) * articles_per_page
            end_idx = min(start_idx + articles_per_page, total_articles)
            page_articles = all_articles[start_idx:end_idx]

            # åˆ†é¡µä¿¡æ¯
            pagination = {
                'current_page': page_num,
                'total_pages': total_pages,
                'prev_page': f"page-{page_num - 1}.html" if page_num > 2 else ("index.html" if page_num == 2 else None),
                'next_page': f"page-{page_num + 1}.html" if page_num < total_pages else None
            }

            # æ¸²æŸ“é¡µé¢
            page_html = index_template.render(
                articles=page_articles,
                pagination=pagination,
                YEAR=datetime.now().year
            )

            # å†™å…¥æ–‡ä»¶
            if page_num == 1:
                filename = "index.html"
            else:
                filename = f"page-{page_num}.html"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(page_html)

            print(f"âœ“ ç”Ÿæˆé¡µé¢: {filename} ({len(page_articles)} ç¯‡æ–‡ç« )")

        print(f"âœ“ ä¸»é å·²ç”Ÿæˆ (å…± {total_pages} é¡µ)")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆä¸»é å¤±æ•—: {e}")

    # 2. ç”Ÿæˆæœç´¢é 
    if os.path.exists('templates/search.html'):
        print("ç”Ÿæˆæœç´¢é ...")
        try:
            search_template = env.get_template('search.html')
            search_html = search_template.render(
                YEAR=datetime.now().year,
                label_info=label_info
            )
            with open("search.html", "w", encoding="utf-8") as f:
                f.write(search_html)
            print("âœ“ æœç´¢é å·²ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæœç´¢é å¤±æ•—: {e}")
    else:
        print("â­ï¸ è·³éæœç´¢é ç”Ÿæˆ (æ¨¡æ¿ä¸å­˜åœ¨)")

    # 3. ç”Ÿæˆé—œæ–¼é 
    if os.path.exists('templates/about.html'):
        print("ç”Ÿæˆé—œæ–¼é ...")
        try:
            about_template = env.get_template('about.html')
            about_html = about_template.render(YEAR=datetime.now().year)
            with open("about.html", "w", encoding="utf-8") as f:
                f.write(about_html)
            print("âœ“ é—œæ–¼é å·²ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆé—œæ–¼é å¤±æ•—: {e}")
    else:
        print("â­ï¸ è·³éé—œæ–¼é ç”Ÿæˆ (æ¨¡æ¿ä¸å­˜åœ¨)")

    # 4. ç”Ÿæˆ SEO æ–‡ä»¶
    print("ç”Ÿæˆ SEO æ–‡ä»¶...")
    generate_sitemap(issues)
    generate_robots_txt()


    # è¤‡è£½éœæ…‹è³‡æº
    print("è¤‡è£½éœæ…‹è³‡æº...")
    os.makedirs("static", exist_ok=True)

    static_files = ["style.css"]
    for f in static_files:
        src = os.path.join("templates", f)
        if os.path.exists(src):
            try:
                shutil.copy(src, f"static/{f}")
                print(f"âœ“ è¤‡è£½éœæ…‹æ–‡ä»¶: {f}")
            except Exception as e:
                print(f"âš ï¸ è¤‡è£½ {f} å¤±æ•—: {e}")
        else:
            print(f"â­ï¸ è·³é {f} (æ–‡ä»¶ä¸å­˜åœ¨)")

    print("\nğŸ‰ ä»»å‹™å®Œæˆï¼")
    print(f"å·²ç”Ÿæˆ {len(issues)} ç¯‡æ–‡ç« ")
    print(f"éœæ…‹è³‡æºä½ç½®: static/")
    print(f"æ–‡ç« ç›®éŒ„: {ARTICLES_DIR}/")

if __name__ == "__main__":
    main()
